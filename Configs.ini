[TEXT_TRANSFORMATION]
image_width = 36
image_height = 36
start = 0
end = 200
font_size = 15
sliding_window_stride = 1
current_encoding = image_encoding
[PREPROCESSING]
do_preprocessing = False
[DATA]
font_file = /data/daif/Arabic-Document-Classification/Deep_learning/data/ae_Petra.ttf
current_dataset=Wikipedia_Title_Dataset
training_csv_file_name = /data/daif/Arabic-Document-Classification/Deep_learning/data/ar_training.txt
testing_csv_file_name = /data/daif/Arabic-Document-Classification/Deep_learning/data/ar_testing.txt
chars_csv_file_name = /data/daif/Arabic-Document-Classification/Deep_learning/data/Arabic_Letters.csv
results_file = data/results.csv
log_file =data/app.log
checkpoint_file = /checkpoint15/
aravec_file = /data/daif/Arabic-Document-Classification/Deep_learning/data/full_grams_cbow_300_wiki.mdl
col_names = class,title
sentence_ column = title
target_column = class
char_df_column = Char
chars_out_file = /data/daif/Arabic-Document-Classification/Deep_learning/data/chars.txt
load_serialized = False
serialize = True
[ARCHITECTURE]
current_network = SelfAttentive
[TRAINING]
onehot=False
train_percent = 0.8
batch_size = 64
epoch = 50
triger_epoch = 1
learning_rate = 0.001
weight_decay = 0.0001
loader_job = 5
device_id = cuda:1
trainer_logs = data/trainer_logs_no_pooling/
beta = 0.99
gamma = 2.0
balanced_loss = False
balanced_iterator = False
batch_balancing = False
loss_function = softmax
[DEBUG]
limit_data = False
data_limit = 200000
[CECLCNN]
num_chars = 60
char_enc_dim = 128
ksize = 3
feature_maps = 512
fc_layer_size = 2048
dropout = 0.1
wildcard_ratio = 0.1
[CE]
current_encoder = CharacterEncoder
chars = 1
feature_maps = 8
fc1_in = 400
fc1_out = 128
fc2_in = 128
k_size = 3
dropout = 0.1
[TESTING]
n_processes = 0
[CNN_Sentence]
in_channels = 1
num_feature_maps = 100
word_dim = 300
FILTER_SIZES = 3,4,5
dropout=0.7
mode=unichannel
[GRU]
rnn_model = GRU
embed_size = 128
hidden_size = 128
num_layers = 3
wildcard_ratio = 0.1
use_last =  False
[RNNImageEncoder]
num_layers = 3
rnn_model = GRU
embed_size = 128
hidden_size = 128
attention_hidden = 128
num_hops=1
bidirectional = True
wildcard_ratio = 0.1
use_last =  False
fc1_out = 32
